# Copyright (c) 2014-present Snowplow Analytics Ltd. All rights reserved.
#
# This software is made available by Snowplow Analytics, Ltd.,
# under the terms of the Snowplow Limited Use License Agreement, Version 1.1
# located at https://docs.snowplow.io/limited-use-license-1.1
# BY INSTALLING, DOWNLOADING, ACCESSING, USING OR DISTRIBUTING ANY PORTION
# OF THE SOFTWARE, YOU AGREE TO THE TERMS OF SUCH LICENSE AGREEMENT.

{

  "license" {
    "accept": "false"
    "accept": ${?ACCEPT_LIMITED_USE_LICENSE}
  }

  "gcpUserAgent": {
    "productName": "Snowplow OSS"
    "productName": ${?GCP_USER_AGENT_PRODUCT_NAME}
    "productVersion": "lake-loader"
  }

  "output": {
    "good": {
      "type": "Delta"
      "type": ${?packaging.output.good.type}

      "deltaTableProperties": {
        "delta.logRetentionDuration": "interval 1 days"
        "delta.dataSkippingStatsColumns": "load_tstamp,collector_tstamp,derived_tstamp,dvce_created_tstamp,true_tstamp"
        "delta.checkpointInterval": "50"
      }

      "icebergTableProperties": {
        "write.spark.accept-any-schema": "true"
        "write.object-storage.enabled": "true"
        "write.metadata.delete-after-commit.enabled": "true"
        "write.metadata.compression-codec": "gzip"
        "write.metadata.metrics.max-inferred-column-defaults": "0"
        "write.metadata.metrics.column.load_tstamp": "full"
        "write.metadata.metrics.column.collector_tstamp": "full"
        "write.metadata.metrics.column.derived_tstamp": "full"
        "write.metadata.metrics.column.dvce_created_tstamp": "full"
        "write.metadata.metrics.column.true_tstamp": "full"
        "commit.retry.num-retries": "20"
      }

      "icebergWriteOptions": {
        "merge-schema": "true"
        "check-ordering": "false"
        "distribution-mode": "none"
      }

      "hudiTableProperties": {
        "hoodie.table.name": "events"
        "hoodie.table.keygenerator.class": "org.apache.hudi.keygen.TimestampBasedKeyGenerator"
        "hoodie.table.partition.fields": "load_tstamp"
        "hoodie.table.precombine.field": "load_tstamp"
        "hoodie.keygen.timebased.timestamp.scalar.time.unit": "microseconds"
        "hoodie.keygen.timebased.output.dateformat": "yyyy-MM-dd"
      }

      "hudiWriteOptions": {
        # -- This loader works most efficiently with BULK_INSERT instead of INSERT
        "hoodie.datasource.write.operation": "BULK_INSERT"

        # -- Record key and partition settings. Chosen to be consistent with `hudiTableOptions`.
        "hoodie.keygen.timebased.timestamp.type": "SCALAR"
        "hoodie.keygen.timebased.output.dateformat": "yyyy-MM-dd"
        "hoodie.datasource.write.partitionpath.field": "load_tstamp"
        "hoodie.write.set.null.for.missing.columns": "true"
        "hoodie.metadata.index.column.stats.column.list": "load_tstamp,collector_tstamp,derived_tstamp,dvce_created_tstamp,true_tstamp"
        "hoodie.metadata.index.column.stats.enable": "true"
        "hoodie.datasource.write.keygenerator.consistent.logical.timestamp.enabled": "true"

        # -- Configures how Hudi manages the timeline
        "hoodie.embed.timeline.server": "true"
        "hoodie.embed.timeline.server.reuse.enabled": "true"
        "hoodie.filesystem.view.incr.timeline.sync.enable": "true"
        "hoodie.filesystem.view.type": "SPILLABLE_DISK"

        # -- Hive sync is disabled by default. But if someone does enable it then these are helpful settings:
        "hoodie.datasource.hive_sync.partition_extractor_class": "org.apache.hudi.hive.MultiPartKeysValueExtractor"
        "hoodie.datasource.hive_sync.partition_fields": "load_tstamp_date"
        "hoodie.datasource.hive_sync.support_timestamp": "true"

        # -- Clustering: Every 4 commits, rewrite the latest parquet files to boost their size.
        "hoodie.clustering.inline": "true"
        "hoodie.clustering.plan.partition.filter.mode": "RECENT_DAYS"
        "hoodie.clustering.plan.strategy.daybased.lookback.partitions": "1"
        "hoodie.clustering.plan.strategy.target.file.max.bytes": "500000000" # 500 MB
        "hoodie.clustering.plan.strategy.small.file.limit": "100000000" # 100 MB
        "hoodie.clustering.inline.max.commits": "4" # Should be smaller than the ratio `target.file.max.bytes` / `small.file.limit`
        "hoodie.clustering.plan.strategy.single.group.clustering.enabled": "false"

        # -- Parallelism. This loader works best when we prevent Hudi from ever running >1 task at a time
        "hoodie.file.listing.parallelism": "1"
        "hoodie.metadata.index.bloom.filter.parallelism": "1"
        "hoodie.metadata.index.column.stats.parallelism": "1"
        "hoodie.clustering.max.parallelism": "1"
        "hoodie.finalize.write.parallelism": "1"
        "hoodie.markers.delete.parallelism": "1"
        "hoodie.rollback.parallelism": "1"
        "hoodie.upsert.shuffle.parallelism": "1"
        "hoodie.bloom.index.parallelism": "1"
        "hoodie.insert.shuffle.parallelism": "1"
        "hoodie.archive.delete.parallelism": "1"
        "hoodie.cleaner.parallelism": "1"
        "hoodie.clustering.plan.strategy.max.num.groups": "1"
      }

      "catalog": {
        "type": "Hadoop"
        "options": {}
      }
    }
  }

  "inMemBatchBytes": 50000000
  "cpuParallelismFraction": 0.75
  "windowing": "5 minutes"
  "numEagerWindows": 1
  "spark": {
    "taskRetries": 3
    "conf": {
      "spark.ui.enabled": "false"
      "spark.local.dir": "/tmp" # This is the default but it's important for us
      "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
      "spark.memory.fraction": "0.3" # Decreased from the Spark default to prevent OOMs
      "spark.sql.parquet.outputTimestampType": "TIMESTAMP_MICROS"
      "spark.sql.parquet.datetimeRebaseModeInWrite": "CORRECTED"
      "spark.memory.storageFraction": "0"
      "spark.databricks.delta.autoCompact.enabled": "false"
      "spark.scheduler.mode": "FAIR"
      "spark.sql.adaptive.enabled": "false" # False gives better performance on the type of shuffle done by Lake Loader
    }
    "gcpUserAgent": ${gcpUserAgent}
  }

  "retries": {
    "setupErrors": {
      "delay": "30 seconds"
    }
    "transientErrors": {
      "delay": "1 second"
      "attempts": 5
    }
  }

  "http": {
    "client": ${snowplow.defaults.http.client}
  }

  "skipSchemas": []
  "respectIgluNullability": true
  "exitOnMissingIgluSchema": true

  "monitoring": {
    "metrics": {
      "statsd": ${snowplow.defaults.statsd}
      "statsd": {
        "prefix": "snowplow.lakeloader"
      }
    }
    "webhook": ${snowplow.defaults.webhook}
    "sentry": {
      "tags": {
      }
    }
    "healthProbe": {
      "port": 8000
      "unhealthyLatency": "1 minute"
    }
  }

  "telemetry": ${snowplow.defaults.telemetry}
}
